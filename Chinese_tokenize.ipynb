{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chinese_tokenize.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyqLfaOVdu3PoYYEhR3BPx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StanleyLiangYork/NLP_deepLearning/blob/master/Chinese_tokenize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRv0Idx9t2BU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "098692d9-949e-4a00-d9d8-4bc01fce4df8"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('red_bean.txt'):\n",
        "    !gsutil cp gs://pet-detect-239118/red_bean.txt ./red_bean.txt\n",
        "\n",
        "if not os.path.exists('sino_punc.txt'):\n",
        "    !gsutil cp gs://pet-detect-239118/sino_punc.txt ./sino_punc.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://pet-detect-239118/red_bean.txt...\n",
            "/ [1 files][  306.0 B/  306.0 B]                                                \n",
            "Operation completed over 1 objects/306.0 B.                                      \n",
            "Copying gs://pet-detect-239118/sino_punc.txt...\n",
            "/ [1 files][  226.0 B/  226.0 B]                                                \n",
            "Operation completed over 1 objects/226.0 B.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qArgTfe6p6DB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8c1861e2-5dca-4bf2-f9c8-48d7b5d87c44"
      },
      "source": [
        "import re\n",
        "from random import shuffle\n",
        "\n",
        "\n",
        "redbean_file = open('red_bean.txt','r').read()\n",
        "redbean = redbean_file.split('\\n')[0]\n",
        "print('The original text is:\\n',redbean, sep='')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The original text is:\n",
            "紅豆生南國，春來發幾枝?願君多採擷，此物最相思。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv3eXl-lqFIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ad85c295-19ba-4904-f04a-7f5c80f49af1"
      },
      "source": [
        "sino_punc = open('sino_punc.txt','r').read()\n",
        "redbean_cleaned = re.sub(sino_punc, '', redbean)\n",
        "print('After removing punctuation:\\n',redbean_cleaned)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After removing punctuation:\n",
            " 紅豆生南國春來發幾枝願君多採擷此物最相思\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kTA1v2oqHJ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1b5486da-1c20-4108-c240-f3cc93531cf8"
      },
      "source": [
        "tokens = list(redbean_cleaned)\n",
        "#shuffle(tokens)\n",
        "output = '\\n'.join(tokens[:5])\n",
        "print(tokens[:5])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['紅', '豆', '生', '南', '國']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dkdpd1TqJ1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summ_file = open('summary_redbean.txt','w')\n",
        "summ_file.write(output)\n",
        "summ_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRNX_yzTuypr",
        "colab_type": "text"
      },
      "source": [
        "Use the Chinese Tokenizer Jieba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8cXZZjcrBy_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c020a2d5-7e65-49d2-8950-9fd1dfa96d15"
      },
      "source": [
        "import jieba\n",
        "\n",
        "news_file = open('red_bean.txt','r').read()\n",
        "news = news_file.split('\\n')[0]\n",
        "# print(type(news))\n",
        "tokens = jieba.cut_for_search(news)\n",
        "print(', '.join(tokens))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "紅豆, 生南國, ，, 春來, 發幾枝, ?, 願君, 多, 採擷, ，, 此物, 最, 相思, 。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyPBrx70rUZm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "99108b05-bb4f-49bf-d85d-d7161cf87138"
      },
      "source": [
        "line = str('合谷，太冲, 足三里, 阳陵泉, 消渴症，高血压')\n",
        "#print(type(line))\n",
        "tokens = jieba.cut_for_search(line)\n",
        "print(', '.join(tokens))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "合谷, ，, 太冲, ,,  , 三里, 足三里, ,,  , 阳陵泉, ,,  , 消渴, 症, ，, 血压, 高血压\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbOrH1dZvVQp",
        "colab_type": "text"
      },
      "source": [
        "Tokenize by unigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfFKCG9bsrW3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "6f95957f-3e0e-4cfb-a798-b015b4d812ca"
      },
      "source": [
        "tokens = list(line)\n",
        "print(tokens)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['合', '谷', '，', '太', '冲', ',', ' ', '足', '三', '里', ',', ' ', '阳', '陵', '泉', ',', ' ', '消', '渴', '症', '，', '高', '血', '压']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}